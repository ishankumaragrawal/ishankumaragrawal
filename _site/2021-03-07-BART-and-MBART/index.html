<!DOCTYPE html>
<html lang="en">
<!-- Beautiful Jekyll 5.0.0 | Copyright Dean Attali 2020 -->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  

  

  <title>BART and MBART</title>

  
  <meta name="author" content="Ishan Kumar">
  

  <meta name="description" content="Transformers for seq2seq tasks">

  
  <meta name="theme-color" content="#0085A1">
  

  
  <meta name="keywords" content="Ishan,ishan,kumar,Kumar,Ishan Kumar,ishan kumar, IIT Roorkee, DSG, Ishan Kumar IITR, Ishan Kumar IIT Roorkee, Ishan Kumar Agrawal, Ishan Kumar Aggrawal, Ishan Kumar Agarwal, ishan kumar agrawal, Ishan K Agrawal">
  

  <link rel="alternate" type="application/rss+xml" title="Ishan Kumar" href="http://localhost:4001/feed.xml">

  

  

  
<!-- Google Analytics -->
<script>
  (function (i, s, o, g, r, a, m) {
    i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
      (i[r].q = i[r].q || []).push(arguments)
    }, i[r].l = 1 * new Date(); a = s.createElement(o),
      m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
  })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');
  ga('create', 'UA-195294169-1', 'auto');
  ga('send', 'pageview');
</script>
<!-- End Google Analytics -->



  
    
      
  <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">


    
      
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css">


    
      
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic">


    
      
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800">


    
  

  
    
      <link rel="stylesheet" href="/assets/css/bootstrap-social.css">
    
      <link rel="stylesheet" href="/assets/css/beautifuljekyll.css">
    
  

  

  
  
  

  

  
  <meta property="og:site_name" content="Ishan Kumar">
  <meta property="og:title" content="BART and MBART">
  <meta property="og:description" content="Transformers for seq2seq tasks">

  
  <meta property="og:image" content="http://localhost:4001/assets/img/BART/BART_Arch.png">
  

  
  <meta property="og:type" content="article">
  <meta property="og:article:author" content="Ishan Kumar">
  <meta property="og:article:published_time" content="2021-03-07T00:00:00+00:00">
  <meta property="og:url" content="http://localhost:4001/2021-03-07-BART-and-MBART/">
  <link rel="canonical" href="http://localhost:4001/2021-03-07-BART-and-MBART/">
  

  
  <meta name="twitter:card" content="summary_large_image">
  
  <meta name="twitter:site" content="@ishankumar216">
  <meta name="twitter:creator" content="@ishankumar216">

  <meta property="twitter:title" content="BART and MBART">
  <meta property="twitter:description" content="Transformers for seq2seq tasks">

  
  <meta name="twitter:image" content="http://localhost:4001/assets/img/BART/BART_Arch.png">
  

  


  

  

</head>


<body>

  


  <nav class="navbar navbar-expand-xl navbar-light fixed-top navbar-custom top-nav-regular"><a class="navbar-brand" href="http://localhost:4001/">Ishan Kumar</a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  <div class="collapse navbar-collapse" id="main-navbar">
    <ul class="navbar-nav ml-auto">
          <li class="nav-item">
            <a class="nav-link" href="/index">About Me</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="/blogs">Blogs</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="/projects">Projects</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="https://drive.google.com/file/d/1pyML__Wo-llOx6Qv6q5yaPmS2vCP9lWX/view?usp=sharing">CV</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="/courses">Courses</a>
          </li></ul>
  </div>

  

  
    <div class="avatar-container">
      <div class="avatar-img-border">
        <a href="http://localhost:4001/">
          <img alt="Navigation bar avatar" class="avatar-img" src="/assets/img/photo_ishan.png" />
        </a>
      </div>
    </div>
  

</nav>


  <!-- TODO this file has become a mess, refactor it -->






  <div id="header-big-imgs" data-num-img=1
    
    
    
      
      data-img-src-1="http://localhost:4001/assets/img/BART/BART_Cover.jpg"
    
    
    
  ></div>


<header class="header-section has-img">

<div class="big-img intro-header">
  <div class="container-md">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
        <div class="post-heading">
          <h1>BART and MBART</h1>
          
            
              <h2 class="post-subheading">Transformers for seq2seq tasks</h2>
            
          

          
            <span class="post-meta">Posted on March 7, 2021</span>
            
            
          
        </div>
      </div>
    </div>
  </div>
  <span class='img-desc'></span>
</div>

<div class="intro-header no-img">
  <div class="container-md">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
        <div class="post-heading">
          <h1>BART and MBART</h1>
          
            
              <h2 class="post-subheading">Transformers for seq2seq tasks</h2>
            
          

          
            <span class="post-meta">Posted on March 7, 2021</span>
            
            
          
        </div>
      </div>
    </div>
  </div>
</div>
</header>





<div class=" container-md ">
  <div class="row">
    <div class=" col-xl-8 offset-xl-2 col-lg-12 offset-lg-1 ">

      

      

      <article role="main" class="blog-post">
        <p>BART Paper - <a href="https://arxiv.org/pdf/1910.13461.pdf">BART: Denoising Sequence-to-Sequence Pre-training for Natural
Language Generation, Translation, and Comprehension</a></p>

<p>In a Single line it is a <strong>seq2seq Tranformer</strong> (thus much more generalised form of BERT and GPT) and is trained in a <strong>Denoising Autoencoder Manner</strong> (similar to BERT’s Masked Language model). 
<img src="../assets/img/BART/BART_Arch.png" alt="BART Model" class="mx-auto d-block" /></p>

<p>Through this work it can be seen that the pre-train finetune method which has become so popular since BERT, is also applicable for seq2seq tasks like Machine Translation, Question Answering (Generative) while not loosing out on comprehension tasks like Extractive QA.</p>

<p>Existing Tranformers are mostly trained on Mask Language Modelling which are also auto encoders of sort that are trained to reconstruct text with a random subset of words masked out. They work on a particular type of end task (like Span Prediction, Generation etc.). 
BART is model which is a denoising autoencoder built with a seq2seq model. 
This makes it applicable to a much wider variety of tasks.
Can be seen as a seq2seq model with <strong>BERT like encoder</strong> and <strong>GPT like decoder</strong> (‘like’ is important as it is similar but not same, so you cant just plug both of them together and call it BART).</p>

<p><strong>Why can’t we use BERT for these tasks?</strong></p>

<p>BERT is a bidirectional encoder- Meaning it takes in a sentence (with a Masked out tokens), sees the word around it and predicts the word which should replace them. The issue is it cant generate new words as it is not working in an Autoregressive Manner (like RNNs for example).
It can be made to work in a LM fashion by ensuring mask is at end and repeatedly sending the sentence into BERT after each prediction check out UniLM. But since here in each case it doesnt get context from the right side of mask the performance will deteriorate and inference time will be very high.</p>

<p><strong>So then why not GPT?</strong></p>

<p>GPT - Works in an autoregressive way. Hence it can be used for LM but doesnt take into account the input sentence very well (no Bidirectionality).
Bidirectional Nature of BERT is special because there is interactions between left and right context words. Prev methods like BiRNN and ELMo used to concatenate left only and right only representations hence no interactions was present between these features.</p>

<p><strong>Architecture of BART</strong></p>

<p>6 layers in both encoder and decoder.
Major changes from BERT - Each layer of Decoder performs cross attention wwith Final hidden Layer of encoder (similar to seq2seq). The feedforward layers are removed from Encoder side, which just increases the number of params in BART by 10% despite having an arch roughly twice the size.</p>

<p><strong>Corruptions used in BART</strong> - 
The masking or corruptions in the input data are very interesting, I feel that simple seq2seq models should also be trained in some manner on these.
<img src="../assets/img/BART/Corruptions.png" alt="All the corruptions used for Pre-training, image from the paper" class="mx-auto d-block" /></p>

<p>I have included a code to do the Masking corruption here</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">mask_tokens</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">vocab_len</span><span class="p">,</span> <span class="n">mask_token</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">mask_prob</span> <span class="o">=</span> <span class="mf">0.7</span><span class="p">):</span>
    <span class="s">"""
    Converts 70% of Input to MASK 15% random other tokens 15% original
    """</span>
    
    <span class="n">labels</span> <span class="o">=</span> <span class="nb">input</span><span class="p">.</span><span class="n">clone</span><span class="p">()</span>
    <span class="n">probability_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">full</span><span class="p">(</span><span class="n">labels</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">mask_prob</span><span class="p">)</span>
    
    <span class="c1">#This line prevents any special tokens like &lt;START&gt; from getting converted into a Mask.
</span>    <span class="c1">#If you dont have any such you can comment it out and if you do, you can pass a mask(boolean) as the arguement to the func.
</span>    <span class="n">probability_matrix</span><span class="p">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">+</span><span class="p">[</span><span class="mi">1</span><span class="p">]).</span><span class="nb">bool</span><span class="p">(),</span> <span class="n">value</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>

    <span class="c1">#Indices of the Masks
</span>    <span class="n">masked_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">bernoulli</span><span class="p">(</span><span class="n">probability_matrix</span><span class="p">).</span><span class="nb">bool</span><span class="p">()</span>
    <span class="n">labels</span><span class="p">[</span><span class="o">~</span><span class="n">masked_indices</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">100</span>  <span class="c1">##Loss is only computed on masked Tokens
</span>    
    
    <span class="n">indices_replaced</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">bernoulli</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">full</span><span class="p">(</span><span class="n">labels</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">mask_prob</span><span class="p">)).</span><span class="nb">bool</span><span class="p">()</span> <span class="o">&amp;</span> <span class="n">masked_indices</span>
    <span class="nb">input</span><span class="p">[</span><span class="n">indices_replaced</span><span class="p">]</span> <span class="o">=</span> <span class="n">mask_token</span>

    <span class="c1">#Replacing the half of the non masked ones with random tokens
</span>    <span class="n">indices_random</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">bernoulli</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">full</span><span class="p">(</span><span class="n">labels</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)).</span><span class="nb">bool</span><span class="p">()</span> <span class="o">&amp;</span> <span class="n">masked_indices</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">indices_replaced</span>
    <span class="n">random_words</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">),</span> <span class="n">labels</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>
    <span class="nb">input</span><span class="p">[</span><span class="n">indices_random</span><span class="p">]</span> <span class="o">=</span> <span class="n">random_words</span><span class="p">[</span><span class="n">indices_random</span><span class="p">]</span>

    <span class="k">return</span> <span class="nb">input</span><span class="p">,</span> <span class="n">labels</span>
</code></pre></div></div>
<hr />
<p>Sample Input and Outputs</p>

<table>
  <thead>
    <tr>
      <th>Original Sent</th>
      <th>Updated Sent</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>At that moment he wasn’t listening to music, he was living an experience.</td>
      <td>At [MASK] [MASK] [MASK] wasn’t [MASK] [MASK] music, [MASK] was living [MASK] experience.</td>
    </tr>
    <tr>
      <td>It was difficult for Mary to admit that most of her workout consisted of exercising poor judgment.</td>
      <td>It [MASK] difficult for Mary to admit At most [MASK] [MASK] workout [MASK] [MASK] exercising of judgment.</td>
    </tr>
    <tr>
      <td>Nobody has encountered an explosive daisy and lived to tell the tale.</td>
      <td>the has encountered an explosive daisy difficult [MASK] to tell the [MASK]</td>
    </tr>
  </tbody>
</table>

<h2 id="tasks-on-which-bart-can-be-finetuned-">Tasks on which BART can be finetuned-</h2>

<ol>
  <li>
    <p><strong>Sequence Classifiation Tasks</strong> - Like Paragraph level Sentiment analysis. In BART there is also a [CLS] token appended to the <strong>end</strong>. We can use a Fully connected layer to classify corresponding to the output of this end [CLS] token. Its appended at the end so that it can get attention scores from all prev words (Decoder moves in a left to right manner unlike BERT).</p>
  </li>
  <li>
    <p><strong>Token Classification Tasks</strong> - Like SQuAD QA. Entire Top Hidden state of Decoder for Classification</p>
  </li>
  <li>
    <p><strong>Generative Tasks</strong> - Generative QA and Summarisation etc. Possible since decoder works in Autoregressive manner.</p>
  </li>
  <li>
    <p><strong>Machine Translation</strong> - Either you can train the entire arch from scratch or use pretrained BART as a pretrained Decoder. This can be done by replacing the embedding layer in BART therefore learning a mapping from the new language to a shared space which BART’s decoder understands. Also see <strong>mBART</strong> explained below</p>
  </li>
</ol>

<p><img src="../assets/img/BART/BART_Perf.png" alt="BART's Performance in comparison with other Transformers" class="mx-auto d-block" /></p>

<ul>
  <li>
    <p>On sequence comprehension Tasks like SQuAD turns out the best BART model (trained using Text Infilling) beats the BERT base model (F1 Scores 90.8 vs 88.5) which means the ability to work on Generative tasks didnt come at a cost of drop in comprehension accuracy.</p>
  </li>
  <li>
    <p>Pre training tasks matter a lot in determining accuracy of models. Some form of Token Masking is necessary, models which are trained only on tasks like Permuting Sentences perform poorly.</p>
  </li>
</ul>

<p>Glue - The General Language Understanding Evaluation (GLUE) benchmark is a collection of nine different language understanding tasks.
Squad -</p>
<ol>
  <li>v1.1 is a collection of 100,000 crowdsourced question/answer pairs drawn from Wikipedia.</li>
  <li>v2.0, introduced in 2018, and builds on this with 50,000 unanswerable questions designed to look like answerable ones. To perform well, the NLP model must determine when the correct answer is not available.</li>
</ol>

<h2 id="mbart">mBART</h2>
<p>Paper - <a href="https://arxiv.org/abs/2001.08210">Multilingual Denoising Pre-training for Neural Machine Translation</a></p>

<p>Its a <strong>seq2seq denoising autoencoder</strong> pre-trained on <strong>large scale monolingual corpora in many languages</strong>.
In short it is BART applied to a large scale monolingual corpora of many different languages. Input is noised and it learns to reconstruct itself.
Given that it has 680M parameters the task of <strong>Learning Semantics of Language</strong> and <strong>Generating coherent sentences</strong> that too in <strong>25+ languages</strong> is a very impressive feat.</p>

<p>This acts as a <strong>initialisation</strong> for NMT tasks, the model downloaded from their website won’t work very well right out of the box. Now using this pretrained mBART you can train on the paired data between the languages you want to translate, but thats not it, you can also work with languages lacking a paired set.</p>

<p>So this would mean that it <strong>may have learnt</strong> some semantics of the language it saw during pretraining?
But this is not the case-
It improves performance on Languages not included in the Pre-training set also. This suggests that the pre-training steps actually make the Transformer learn some <strong>language universal aspects</strong>.</p>

<p><img src="../assets/img/BART/No_Pretrain.png" alt="mBART's performance on unseen languages" class="mx-auto d-block" /></p>

<p><strong>Unsupervised Methods for MT</strong>
So for best results there needs to exist a parallel corpus of Source Text - Target Text, but if that is not possible there are some workarounds the authors mention</p>
<ol>
  <li>
    <p>When no Paired text exists. In that case <strong>backtranslation</strong> can be used, the process is best understood by this image.</p>
  </li>
  <li>
    <p>Parallel corpora exists for both, but not with each other. Zero Shot Transfer also works.</p>
  </li>
</ol>


      </article>

      
        <div class="blog-tags">
          <span>Tags:</span>
          
            <a href="/tags#Papers I read">Papers I read</a>
          
            <a href="/tags#Multi-lingual NLP">Multi-lingual NLP</a>
          
            <a href="/tags#NLP">NLP</a>
          
        </div>
      

      

      

      <ul class="pagination blog-pager">
        
        
        <li class="page-item next">
          <a class="page-link" href="/2021-03-07-Skin-Color-Segmentation-Methods/" data-toggle="tooltip" data-placement="top" title="Skin Segmentation in Images">Next Post &rarr;</a>
        </li>
        
      </ul>
      
  
  
  

  




    </div>
  </div>
</div>


  <footer>
  <div class="container-md beautiful-jekyll-footer">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
      <ul class="list-inline text-center footer-links"><li class="list-inline-item">
    <a href="mailto:ishankumar216@gmail.com" title="Email me">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fas fa-envelope fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">Email me</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://www.facebook.com/ishan.kumar.790" title="Facebook">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-facebook fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">Facebook</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://github.com/Ishan-Kumar2" title="GitHub">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-github fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">GitHub</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://twitter.com/ishankumar216" title="Twitter">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-twitter fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">Twitter</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://linkedin.com/in/ishan-k216" title="LinkedIn">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-linkedin fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">LinkedIn</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://scholar.google.com/https://scholar.google.com/citations?user=Lp0-GgIAAAAJ&hl=en" title="Google Scholar">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fa fa-graduation-cap fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">Google Scholar</span>
    </a>
  </li><li class="list-inline-item">
    <a href="https://medium.com/@ishankumar216" title="Medium">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-medium fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">Medium</span>
    </a>
  </li></ul>

      
      <p class="copyright text-muted">
      
        Ishan Kumar
        &nbsp;&bull;&nbsp;
      
      2023

      

      
      </p>
      <p class="theme-by text-muted">
        Powered by
        <a href="https://beautifuljekyll.com">Beautiful Jekyll</a>
      </p>
      </div>
    </div>
  </div>
</footer>


  
  
    
  <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=" crossorigin="anonymous"></script>


  
    
  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>


  
    
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>


  



  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
      <script src="/assets/js/beautifuljekyll.js"></script>
    
  









</body>
</html>
